{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST Classification Project\n",
    "# 24S2-SC4001 CE/CZ4042: Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries and Dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Device and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import requests\n",
    "import io\n",
    "import gzip\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# GitHub URLs for the Fashion-MNIST dataset\n",
    "github_urls = {\n",
    "    'train_images': \"https://github.com/benjaminyjr17/24S2-SC4001-Fashion-MNIST-Classification/blob/d5d78b36c91724fb63e633c45fa8b5a828939a0c/train-images-idx3-ubyte.gz?raw=true\",\n",
    "    'train_labels': \"https://github.com/benjaminyjr17/24S2-SC4001-Fashion-MNIST-Classification/blob/d5d78b36c91724fb63e633c45fa8b5a828939a0c/train-labels-idx1-ubyte.gz?raw=true\",\n",
    "    'test_images': \"https://github.com/benjaminyjr17/24S2-SC4001-Fashion-MNIST-Classification/blob/d5d78b36c91724fb63e633c45fa8b5a828939a0c/t10k-images-idx3-ubyte.gz?raw=true\",\n",
    "    'test_labels': \"https://github.com/benjaminyjr17/24S2-SC4001-Fashion-MNIST-Classification/blob/d5d78b36c91724fb63e633c45fa8b5a828939a0c/t10k-labels-idx1-ubyte.gz?raw=true\"\n",
    "}\n",
    "\n",
    "# Function to download data from a URL\n",
    "def download_from_url(url):\n",
    "    \"\"\"Download data from a URL and return as bytes\"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading from {url}...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Download successful: {url}\")\n",
    "            return response.content\n",
    "        else:\n",
    "            print(f\"Failed to download from {url}, status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load MNIST format data from a gzipped byte stream\n",
    "def load_mnist_from_bytes(images_bytes, labels_bytes):\n",
    "    \"\"\"Load MNIST format images and labels from byte streams.\"\"\"\n",
    "    \n",
    "    # Read images\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(images_bytes)) as f:\n",
    "        # First 16 bytes are magic_number, n_imgs, n_rows, n_cols\n",
    "        images = np.frombuffer(f.read(), 'B', offset=16)\n",
    "        images = images.reshape(-1, 28, 28).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Read labels\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(labels_bytes)) as f:\n",
    "        # First 8 bytes are magic_number, n_labels\n",
    "        labels = np.frombuffer(f.read(), 'B', offset=8)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Function to load Fashion-MNIST dataset\n",
    "def load_fashion_mnist(use_github=True, data_dir='./data'):\n",
    "    \"\"\"\n",
    "    Load Fashion-MNIST dataset, trying GitHub first and falling back to torchvision.\n",
    "    \n",
    "    Args:\n",
    "        use_github: If True, try to download from GitHub\n",
    "        data_dir: Directory to store the dataset if downloaded via torchvision\n",
    "    \n",
    "    Returns:\n",
    "        train_dataset, test_dataset\n",
    "    \"\"\"\n",
    "    # Try GitHub first if requested\n",
    "    if use_github:\n",
    "        try:\n",
    "            print(\"Attempting to download Fashion-MNIST from GitHub...\")\n",
    "            \n",
    "            # Download files from GitHub\n",
    "            train_images_bytes = download_from_url(github_urls['train_images'])\n",
    "            train_labels_bytes = download_from_url(github_urls['train_labels'])\n",
    "            test_images_bytes = download_from_url(github_urls['test_images'])\n",
    "            test_labels_bytes = download_from_url(github_urls['test_labels'])\n",
    "            \n",
    "            # Check if all downloads were successful\n",
    "            if all([train_images_bytes, train_labels_bytes, test_images_bytes, test_labels_bytes]):\n",
    "                # Load data from byte streams\n",
    "                train_images, train_labels = load_mnist_from_bytes(train_images_bytes, train_labels_bytes)\n",
    "                test_images, test_labels = load_mnist_from_bytes(test_images_bytes, test_labels_bytes)\n",
    "                \n",
    "                # Convert to PyTorch tensors and create datasets\n",
    "                train_images_tensor = torch.tensor(train_images).unsqueeze(1)  # Add channel dimension\n",
    "                train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "                train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "                \n",
    "                test_images_tensor = torch.tensor(test_images).unsqueeze(1)  # Add channel dimension\n",
    "                test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "                test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "                \n",
    "                print(\"Successfully loaded Fashion-MNIST from GitHub.\")\n",
    "                return train_dataset, test_dataset\n",
    "            else:\n",
    "                print(\"Some files failed to download from GitHub.\")\n",
    "                use_github = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading from GitHub: {e}\")\n",
    "            use_github = False\n",
    "    \n",
    "    # Fallback to torchvision\n",
    "    print(\"Loading Fashion-MNIST using torchvision...\")\n",
    "    \n",
    "    # Use torchvision's built-in functionality\n",
    "    train_dataset = datasets.FashionMNIST(\n",
    "        root=data_dir, \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = datasets.FashionMNIST(\n",
    "        root=data_dir, \n",
    "        train=False, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    print(\"Successfully loaded Fashion-MNIST using torchvision.\")\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset, test_dataset = load_fashion_mnist(\n",
    "    use_github=True,  # Try GitHub first\n",
    "    data_dir='./data'  # For torchvision fallback\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load the datasets is already handled in Cell 2\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Data loaders already created in Cell 2\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore and Visualize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class names for Fashion-MNIST\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Function to show images\n",
    "def show_images(images, labels, grid_size=(3, 5), figsize=(15, 9), title=None):\n",
    "    \"\"\"Display a grid of images with their labels.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    images = images.cpu()  # Move to CPU if they're on GPU\n",
    "    grid = make_grid(images, nrow=grid_size[1])\n",
    "    np_grid = grid.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Show the grid\n",
    "    plt.imshow(np_grid * 0.5 + 0.5)  # Denormalize from [-1, 1] to [0, 1]\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Show the labels\n",
    "    if labels is not None:\n",
    "        labels = labels.tolist()\n",
    "        plt.title([class_names[label] for label in labels], fontsize=12)\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show a batch of images\n",
    "show_images(images[:15], labels[:15], title=\"Sample Fashion-MNIST Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Evaluation Metrics and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model performance on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_correct += torch.sum(preds == labels.data)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    epoch_loss = total_loss / len(dataloader.dataset)\n",
    "    epoch_acc = total_correct.double() / len(dataloader.dataset)\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item(), all_preds, all_labels\n",
    "\n",
    "# Function to visualize training history\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs, title=\"Training History\"):\n",
    "    \"\"\"Plot training and validation losses and accuracies.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(train_losses, label='Training Loss')\n",
    "    ax1.plot(val_losses, label='Validation Loss')\n",
    "    ax1.set_title('Loss over Epochs')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot accuracies\n",
    "    ax2.plot(train_accs, label='Training Accuracy')\n",
    "    ax2.plot(val_accs, label='Validation Accuracy')\n",
    "    ax2.set_title('Accuracy over Epochs')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=None, figsize=(10, 8)):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate classification report\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "# Function to train a model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    \"\"\"Train a model and return training history.\"\"\"\n",
    "    # Initialize lists to track metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # Track best model\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        # Progress bar for training\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        \n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(epoch_train_acc.item())\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Deep copy the model if best accuracy\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # Return the trained model and history\n",
    "    history = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'best_acc': best_acc\n",
    "    }\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model #1: Baseline CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN baseline model for Fashion-MNIST.\n",
    "    Architecture:\n",
    "    - Conv1: 3x3 kernels, 32 filters\n",
    "    - Conv2: 3x3 kernels, 64 filters\n",
    "    - MaxPool: 2x2\n",
    "    - Conv3: 3x3 kernels, 128 filters\n",
    "    - MaxPool: 2x2\n",
    "    - Fully Connected: 2048 -> 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BaselineCNN, self).__init__()\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate output size after convolutions and pooling\n",
    "        # Initial size: 28x28 -> After conv1&2 and pool1: 14x14 -> After conv3 and pool2: 7x7\n",
    "        # So final feature map size is 7x7 with 128 channels\n",
    "        self.fc = nn.Linear(7 * 7 * 128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Second block\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Third block\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flatten and pass through fully connected layer\n",
    "        x = x.view(-1, 7 * 7 * 128)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters in the model.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Baseline CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "baseline_model = BaselineCNN().to(device)\n",
    "print(f\"Baseline CNN Parameters: {baseline_model.count_parameters():,}\")\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "baseline_model, baseline_history = train_model(\n",
    "    baseline_model, train_loader, test_loader, criterion, optimizer, device, num_epochs=num_epochs\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(\n",
    "    baseline_history['train_losses'], baseline_history['val_losses'],\n",
    "    baseline_history['train_accs'], baseline_history['val_accs'],\n",
    "    title=\"Baseline CNN Training History\"\n",
    ")\n",
    "\n",
    "# Evaluate the model on test set and plot confusion matrix\n",
    "_, _, baseline_preds, baseline_true = evaluate_model(baseline_model, test_loader, criterion, device)\n",
    "plot_confusion_matrix(baseline_true, baseline_preds, class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model #2: Advanced Technique Implementation (Dilated Convolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN model with dilated convolutions for Fashion-MNIST.\n",
    "    Architecture similar to baseline, but with dilated convolutions.\n",
    "    - Conv1: 3x3 kernels, dilation=1, 32 filters\n",
    "    - Conv2: 3x3 kernels, dilation=2, 64 filters (increased receptive field)\n",
    "    - MaxPool: 2x2\n",
    "    - Conv3: 3x3 kernels, dilation=4, 128 filters (further increased receptive field)\n",
    "    - MaxPool: 2x2\n",
    "    - Fully Connected: 2048 -> 10\n",
    "    \n",
    "    Dilated convolutions increase the receptive field without increasing parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DilatedCNN, self).__init__()\n",
    "        # First convolutional block (standard convolution, dilation=1)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1, dilation=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Second convolutional block with dilation=2\n",
    "        # A 3x3 kernel with dilation=2 has a 5x5 effective receptive field\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=2, dilation=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Third convolutional block with dilation=4\n",
    "        # A 3x3 kernel with dilation=4 has a 9x9 effective receptive field\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=4, dilation=4)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate output size after convolutions and pooling\n",
    "        # Due to padding adjustments for dilation, we maintain feature map sizes\n",
    "        self.fc = nn.Linear(7 * 7 * 128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Second block with dilated convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Third block with further dilated convolution\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flatten and pass through fully connected layer\n",
    "        x = x.view(-1, 7 * 7 * 128)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters in the model.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train the Dilated CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "dilated_model = DilatedCNN().to(device)\n",
    "print(f\"Dilated CNN Parameters: {dilated_model.count_parameters():,}\")\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(dilated_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "dilated_model, dilated_history = train_model(\n",
    "    dilated_model, train_loader, test_loader, criterion, optimizer, device, num_epochs=num_epochs\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(\n",
    "    dilated_history['train_losses'], dilated_history['val_losses'],\n",
    "    dilated_history['train_accs'], dilated_history['val_accs'],\n",
    "    title=\"Dilated CNN Training History\"\n",
    ")\n",
    "\n",
    "# Evaluate the model on test set and plot confusion matrix\n",
    "_, _, dilated_preds, dilated_true = evaluate_model(dilated_model, test_loader, criterion, device)\n",
    "plot_confusion_matrix(dilated_true, dilated_preds, class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Receptive Field Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_receptive_field():\n",
    "    \"\"\"\n",
    "    Visualize the difference in receptive fields between standard and dilated convolutions.\n",
    "    This is a conceptual visualization to illustrate the benefit of dilated convolutions.\n",
    "    \"\"\"\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Standard 3x3 convolution with 3 layers (effective receptive field: 7x7)\n",
    "    # First convolution\n",
    "    standard_field1 = np.zeros((7, 7))\n",
    "    standard_field1[2:5, 2:5] = 1  # 3x3 kernel\n",
    "    \n",
    "    # Second convolution\n",
    "    standard_field2 = np.zeros((7, 7))\n",
    "    standard_field2[1:6, 1:6] = 0.7  # Effective 5x5 field\n",
    "    standard_field2[2:5, 2:5] = 1   # Center is more influential\n",
    "    \n",
    "    # Third convolution\n",
    "    standard_field3 = np.zeros((7, 7))\n",
    "    standard_field3[:, :] = 0.4     # Effective 7x7 field\n",
    "    standard_field3[1:6, 1:6] = 0.7  # Middle part is more influential\n",
    "    standard_field3[2:5, 2:5] = 1   # Center is most influential\n",
    "    \n",
    "    # Dilated convolutions\n",
    "    # First convolution (dilation=1, standard 3x3)\n",
    "    dilated_field1 = np.zeros((11, 11))\n",
    "    dilated_field1[4:7, 4:7] = 1  # 3x3 kernel\n",
    "    \n",
    "    # Second convolution (dilation=2, effective 5x5)\n",
    "    dilated_field2 = np.zeros((11, 11))\n",
    "    dilated_field2[2:9, 2:9] = 0.7  # Effective 7x7 field\n",
    "    dilated_field2[3:8:2, 3:8:2] = 1  # Dilated pixels get more weight\n",
    "    \n",
    "    # Third convolution (dilation=4, effective 9x9)\n",
    "    dilated_field3 = np.zeros((11, 11))\n",
    "    dilated_field3[:, :] = 0.4  # Full field\n",
    "    dilated_field3[1:10, 1:10] = 0.7  # Middle part\n",
    "    dilated_field3[0:11:4, 0:11:4] = 1  # Dilated pixels are most influential\n",
    "    \n",
    "    # Plot the receptive fields\n",
    "    ax1.imshow(standard_field3, cmap='hot')\n",
    "    ax1.set_title('Standard Convolutions\\nReceptive Field After 3 Layers')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(dilated_field3, cmap='hot')\n",
    "    ax2.set_title('Dilated Convolutions\\nReceptive Field After 3 Layers')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.suptitle('Comparison of Receptive Fields', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the receptive field difference\n",
    "visualize_receptive_field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model #3: Efficient Architecture (MobileNet-Inspired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of depthwise separable convolution.\n",
    "    This reduces parameters by factoring standard convolution into:\n",
    "    1. Depthwise convolution (spatial filtering)\n",
    "    2. Pointwise convolution (feature generation)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, stride=1, dilation=1):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        # Depthwise convolution\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=kernel_size, \n",
    "            padding=padding, stride=stride, dilation=dilation, groups=in_channels\n",
    "        )\n",
    "        # Pointwise convolution\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class EfficientCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient CNN architecture inspired by MobileNetV2 for Fashion-MNIST.\n",
    "    Uses depthwise separable convolutions to reduce parameter count.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(EfficientCNN, self).__init__()\n",
    "        # First layer: standard convolution (1x1 input channels)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Second layer: depthwise separable convolution\n",
    "        self.conv2 = DepthwiseSeparableConv(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Third layer: depthwise separable convolution\n",
    "        self.conv3 = DepthwiseSeparableConv(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(7 * 7 * 128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Second block with depthwise separable convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Third block with depthwise separable convolution\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flatten and classify\n",
    "        x = x.view(-1, 7 * 7 * 128)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters in the model.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Train the Efficient CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "efficient_model = EfficientCNN().to(device)\n",
    "print(f\"Efficient CNN Parameters: {efficient_model.count_parameters():,}\")\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(efficient_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "efficient_model, efficient_history = train_model(\n",
    "    efficient_model, train_loader, test_loader, criterion, optimizer, device, num_epochs=num_epochs\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(\n",
    "    efficient_history['train_losses'], efficient_history['val_losses'],\n",
    "    efficient_history['train_accs'], efficient_history['val_accs'],\n",
    "    title=\"Efficient CNN Training History\"\n",
    ")\n",
    "\n",
    "# Evaluate the model on test set and plot confusion matrix\n",
    "_, _, efficient_preds, efficient_true = evaluate_model(efficient_model, test_loader, criterion, device)\n",
    "plot_confusion_matrix(efficient_true, efficient_preds, class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model #4: Data Augmentation Enhancement With MixUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixUpTransform:\n",
    "    \"\"\"\n",
    "    Implementation of MixUp augmentation.\n",
    "    MixUp creates virtual examples by linearly interpolating between pairs of images and labels.\n",
    "    \n",
    "    Reference:\n",
    "    Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017).\n",
    "    mixup: Beyond Empirical Risk Minimization.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Apply mixup to the batch.\n",
    "        Args:\n",
    "            batch: Tuple of (inputs, labels)\n",
    "        Returns:\n",
    "            Tuple of mixed inputs, original labels, mixed labels, and lambda\n",
    "        \"\"\"\n",
    "        inputs, labels = batch\n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        # Sample lambda from Beta distribution\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "        else:\n",
    "            lam = 1.0\n",
    "        \n",
    "        # Create a permutation of the batch indices\n",
    "        index = torch.randperm(batch_size).to(inputs.device)\n",
    "        \n",
    "        # Mix the inputs\n",
    "        mixed_inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
    "        \n",
    "        # Return mixed inputs, original labels, permuted labels, and lambda\n",
    "        return mixed_inputs, labels, labels[index], lam\n",
    "\n",
    "# Function to train with MixUp\n",
    "def train_with_mixup(model, train_loader, val_loader, criterion, optimizer, device, mixup_alpha=0.2, num_epochs=10):\n",
    "    \"\"\"Train a model using MixUp augmentation.\"\"\"\n",
    "    # Create MixUp transform\n",
    "    mixup = MixUpTransform(alpha=mixup_alpha)\n",
    "    \n",
    "    # Initialize lists to track metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # Track best model\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        # Progress bar for training\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train with MixUp]\")\n",
    "        \n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Apply mixup\n",
    "            mixed_inputs, labels_a, labels_b, lam = mixup((inputs, labels))\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(mixed_inputs)\n",
    "            \n",
    "            # Compute loss with mixup\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # For accuracy with mixup, we use the original labels for simplicity\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation phase (without mixup)\n",
    "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(epoch_train_acc.item())\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Deep copy the model if best accuracy\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # Return the trained model and history\n",
    "    history = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'best_acc': best_acc\n",
    "    }\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualize MixUp Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mixup_examples():\n",
    "    \"\"\"Create and show examples of MixUp augmentation.\"\"\"\n",
    "    # Get some random training images\n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Create MixUp transform\n",
    "    mixup = MixUpTransform(alpha=0.2)\n",
    "    \n",
    "    # Create 5 examples with different lambdas\n",
    "    lambdas = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, lam in enumerate(lambdas):\n",
    "        # Force a specific lambda for this demo\n",
    "        np.random.seed(i)  # For reproducibility\n",
    "        \n",
    "        # Apply mixup with fixed lambda\n",
    "        mixed_images, labels_a, labels_b, _ = mixup((images[:10], labels[:10]))\n",
    "        \n",
    "        # Display original images\n",
    "        plt.subplot(len(lambdas), 3, i*3 + 1)\n",
    "        img1 = images[0].numpy().transpose(1, 2, 0)\n",
    "        plt.imshow(img1 * 0.5 + 0.5)\n",
    "        plt.title(f\"Image 1: {class_names[labels[0]]}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(len(lambdas), 3, i*3 + 2)\n",
    "        img2 = images[1].numpy().transpose(1, 2, 0)\n",
    "        plt.imshow(img2 * 0.5 + 0.5)\n",
    "        plt.title(f\"Image 2: {class_names[labels[1]]}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Display mixed image\n",
    "        plt.subplot(len(lambdas), 3, i*3 + 3)\n",
    "        mixed_img = mixed_images[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        plt.imshow(mixed_img * 0.5 + 0.5)\n",
    "        plt.title(f\"Mixed with λ={lam:.1f}\\n\"\n",
    "                  f\"{lam:.1f}×{class_names[labels_a[0]]} + \"\n",
    "                  f\"{1-lam:.1f}×{class_names[labels_b[0]]}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"MixUp Data Augmentation Examples\", fontsize=16)\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "    plt.show()\n",
    "\n",
    "# Show MixUp examples\n",
    "show_mixup_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Train the Best Model With MixUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best performing model architecture from previous experiments\n",
    "# For example, let's use the dilated CNN model\n",
    "mixup_model = DilatedCNN().to(device)\n",
    "print(f\"MixUp Model Parameters: {mixup_model.count_parameters():,}\")\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mixup_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train with MixUp\n",
    "mixup_model, mixup_history = train_with_mixup(\n",
    "    mixup_model, train_loader, test_loader, criterion, optimizer, \n",
    "    device, mixup_alpha=0.2, num_epochs=num_epochs\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(\n",
    "    mixup_history['train_losses'], mixup_history['val_losses'],\n",
    "    mixup_history['train_accs'], mixup_history['val_accs'],\n",
    "    title=\"MixUp Training History\"\n",
    ")\n",
    "\n",
    "# Evaluate the model on test set and plot confusion matrix\n",
    "_, _, mixup_preds, mixup_true = evaluate_model(mixup_model, test_loader, criterion, device)\n",
    "plot_confusion_matrix(mixup_true, mixup_preds, class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Comprehensive Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models_info, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Compare multiple models on various metrics.\n",
    "    Args:\n",
    "        models_info: List of dictionaries with model info\n",
    "        test_loader: DataLoader for the test set\n",
    "        criterion: Loss function\n",
    "        device: Device to run evaluation on\n",
    "    \"\"\"\n",
    "    # Collect results\n",
    "    results = []\n",
    "    \n",
    "    for info in models_info:\n",
    "        model = info['model']\n",
    "        name = info['name']\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss, test_acc, preds, true_labels = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Calculate per-class accuracy\n",
    "        cm = confusion_matrix(true_labels, preds)\n",
    "        per_class_acc = np.diag(cm) / cm.sum(axis=1)\n",
    "        \n",
    "        # Count parameters\n",
    "        num_params = model.count_parameters()\n",
    "        \n",
    "        # Save results\n",
    "        results.append({\n",
    "            'name': name,\n",
    "            'accuracy': test_acc,\n",
    "            'loss': test_loss,\n",
    "            'parameters': num_params,\n",
    "            'per_class_accuracy': per_class_acc,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_model_comparison(results):\n",
    "    \"\"\"Plot comparison of model results.\"\"\"\n",
    "    # Extract data for plotting\n",
    "    names = [r['name'] for r in results]\n",
    "    accuracies = [r['accuracy'] * 100 for r in results]\n",
    "    parameters = [r['parameters'] for r in results]\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot accuracy comparison\n",
    "    ax1.bar(names, accuracies)\n",
    "    ax1.set_ylim([min(accuracies) - 1, max(accuracies) + 1])\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.set_title('Test Accuracy Comparison')\n",
    "    \n",
    "    for i, v in enumerate(accuracies):\n",
    "        ax1.text(i, v + 0.1, f\"{v:.2f}%\", ha='center')\n",
    "    \n",
    "    # Plot parameter count comparison\n",
    "    ax2.bar(names, parameters)\n",
    "    ax2.set_ylabel('Number of Parameters')\n",
    "    ax2.set_title('Model Size Comparison')\n",
    "    \n",
    "    for i, v in enumerate(parameters):\n",
    "        ax2.text(i, v + 1000, f\"{v:,}\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Per-class accuracy comparison\n",
    "    per_class_acc = np.array([r['per_class_accuracy'] * 100 for r in results])\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        offset = (i - len(results) / 2 + 0.5) * width\n",
    "        plt.bar(x + offset, result['per_class_accuracy'] * 100, width, label=result['name'])\n",
    "    \n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Per-Class Accuracy Comparison')\n",
    "    plt.xticks(x, class_names, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare all models\n",
    "models_info = [\n",
    "    {'name': 'Baseline CNN', 'model': baseline_model},\n",
    "    {'name': 'Dilated CNN', 'model': dilated_model},\n",
    "    {'name': 'Efficient CNN', 'model': efficient_model},\n",
    "    {'name': 'MixUp Dilated CNN', 'model': mixup_model},\n",
    "]\n",
    "\n",
    "comparison_results = compare_models(models_info, test_loader, criterion, device)\n",
    "plot_model_comparison(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Model Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(model, data_loader, device, class_names, num_images=25):\n",
    "    \"\"\"\n",
    "    Analyze and visualize model errors.\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: DataLoader with test data\n",
    "        device: Device to run model on\n",
    "        class_names: List of class names\n",
    "        num_images: Number of error images to display\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Find errors\n",
    "            error_indices = (preds != labels).nonzero(as_tuple=True)[0]\n",
    "            \n",
    "            for idx in error_indices:\n",
    "                errors.append({\n",
    "                    'image': inputs[idx].cpu(),\n",
    "                    'true': labels[idx].item(),\n",
    "                    'pred': preds[idx].item(),\n",
    "                    'confidence': F.softmax(outputs[idx], dim=0)[preds[idx]].item()\n",
    "                })\n",
    "                \n",
    "                if len(errors) >= num_images:\n",
    "                    break\n",
    "            \n",
    "            if len(errors) >= num_images:\n",
    "                break\n",
    "    \n",
    "    # Display error images\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i, error in enumerate(errors[:num_images]):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        img = error['image'].numpy().transpose(1, 2, 0)\n",
    "        plt.imshow(img * 0.5 + 0.5)\n",
    "        plt.title(f\"True: {class_names[error['true']]}\\n\"\n",
    "                  f\"Pred: {class_names[error['pred']]}\\n\"\n",
    "                  f\"Conf: {error['confidence']:.2f}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Error Analysis: {model.__class__.__name__}\", fontsize=16)\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "    plt.show()\n",
    "\n",
    "# Analyze errors for the best model\n",
    "best_model = baseline_model  # Change this to the best performing model\n",
    "analyze_errors(best_model, test_loader, device, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Feature Space Visualization With t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_space(model, data_loader, device, class_names, perplexity=30, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Visualize the feature space of a model using t-SNE.\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: DataLoader with test data\n",
    "        device: Device to run model on\n",
    "        class_names: List of class names\n",
    "        perplexity: t-SNE perplexity parameter\n",
    "        n_samples: Number of samples to use for t-SNE\n",
    "    \"\"\"\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    # Create a hook to get features from the penultimate layer\n",
    "    features = []\n",
    "    labels_list = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        features.append(output.view(output.size(0), -1).cpu().numpy())\n",
    "    \n",
    "    # Register the hook on the penultimate layer (before the final FC layer)\n",
    "    # This works for our example models, but might need adjustment for other models\n",
    "    if isinstance(model, BaselineCNN) or isinstance(model, DilatedCNN) or isinstance(model, EfficientCNN):\n",
    "        # Register hook on the output of the last pooling layer\n",
    "        model.pool2.register_forward_hook(hook)\n",
    "    else:\n",
    "        # Generic hook on the last pooling layer (may need adjustment)\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.MaxPool2d):\n",
    "                last_pool = module\n",
    "        \n",
    "        last_pool.register_forward_hook(hook)\n",
    "    \n",
    "    # Extract features\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        for inputs, labels in data_loader:\n",
    "            if count >= n_samples:\n",
    "                break\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            _ = model(inputs)\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "            count += inputs.size(0)\n",
    "    \n",
    "    # Concatenate all features and labels\n",
    "    features = np.concatenate(features[:n_samples // batch_size + 1])\n",
    "    labels_list = np.concatenate(labels_list[:n_samples // batch_size + 1])\n",
    "    \n",
    "    # Limit to n_samples\n",
    "    features = features[:n_samples]\n",
    "    labels_list = labels_list[:n_samples]\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    print(\"Applying t-SNE dimensionality reduction...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    reduced_features = tsne.fit_transform(features)\n",
    "    \n",
    "    # Plot the 2D t-SNE embeddings\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    scatter = plt.scatter(\n",
    "        reduced_features[:, 0], reduced_features[:, 1], \n",
    "        c=labels_list, cmap='tab10', alpha=0.7, s=20\n",
    "    )\n",
    "    \n",
    "    plt.colorbar(scatter, ticks=range(10), label='Class')\n",
    "    \n",
    "    # Create proxy artists for legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor=scatter.cmap(scatter.norm(i)), \n",
    "               markersize=8, label=class_names[i]) \n",
    "        for i in range(10)\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='best')\n",
    "    plt.title(f't-SNE Visualization of Feature Space for {model.__class__.__name__}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize feature space of best model\n",
    "best_model = baseline_model  # Change this to the best performing model\n",
    "visualize_feature_space(best_model, test_loader, device, class_names, n_samples=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Parameter Efficiency vs. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_efficiency_vs_performance(results):\n",
    "    \"\"\"\n",
    "    Plot parameter efficiency vs. performance for all models.\n",
    "    Args:\n",
    "        results: List of dictionaries with model comparison results\n",
    "    \"\"\"\n",
    "    names = [r['name'] for r in results]\n",
    "    accuracies = [r['accuracy'] * 100 for r in results]\n",
    "    parameters = [r['parameters'] / 10**6 for r in results]  # Parameters in millions\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for i, (name, params, acc) in enumerate(zip(names, parameters, accuracies)):\n",
    "        plt.scatter(params, acc, s=100, label=name)\n",
    "        plt.annotate(name, (params, acc), fontsize=10, \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel('Number of Parameters (millions)')\n",
    "    plt.ylabel('Test Accuracy (%)')\n",
    "    plt.title('Model Efficiency: Parameters vs. Accuracy')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add a trend line\n",
    "    from scipy.stats import linregress\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(parameters, accuracies)\n",
    "    x = np.array([min(parameters), max(parameters)])\n",
    "    y = slope * x + intercept\n",
    "    plt.plot(x, y, 'r--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot efficiency vs. performance\n",
    "plot_efficiency_vs_performance(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Class Activation Maps (CAM) for Model Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cam(model, image, target_class, device):\n",
    "    \"\"\"\n",
    "    Generate Class Activation Map for a model and image.\n",
    "    This is a simple implementation and may need adjustment for different models.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        image: Input image tensor [1, 1, H, W]\n",
    "        target_class: Target class index\n",
    "        device: Device to run model on\n",
    "    \n",
    "    Returns:\n",
    "        cam: Class activation map\n",
    "        output: Model output\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # First, remove any existing hooks to avoid conflicts\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, '_forward_hooks'):\n",
    "            module._forward_hooks.clear()\n",
    "    \n",
    "    # Move image to device\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    features = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        features.append(output.detach())\n",
    "    \n",
    "    # Find and register hook on the last convolutional layer\n",
    "    hook_handle = None\n",
    "    \n",
    "    if isinstance(model, BaselineCNN) or isinstance(model, DilatedCNN) or isinstance(model, EfficientCNN):\n",
    "        hook_handle = model.conv3.register_forward_hook(hook_fn)\n",
    "    else:\n",
    "        # Generic hook on the last conv layer (may need adjustment)\n",
    "        last_conv = None\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                last_conv = module\n",
    "        \n",
    "        if last_conv is not None:\n",
    "            hook_handle = last_conv.register_forward_hook(hook_fn)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "    \n",
    "    # Remove the hook after use\n",
    "    if hook_handle is not None:\n",
    "        hook_handle.remove()\n",
    "    \n",
    "    # Get weights for the target class\n",
    "    fc_weights = model.fc.weight[target_class].cpu().data.numpy()\n",
    "    \n",
    "    # Get feature maps from the last convolutional layer\n",
    "    feature_maps = features[0].cpu().data.numpy().squeeze()\n",
    "    \n",
    "    # Use the first C weights where C is the number of channels\n",
    "    num_channels = feature_maps.shape[0]\n",
    "    \n",
    "    # Option 1: Use a slice of weights if we have more weights than channels\n",
    "    if len(fc_weights) > num_channels:\n",
    "        channel_weights = fc_weights[:num_channels]\n",
    "    # Option 2: If weights are exactly for reshaping (which they should be)\n",
    "    elif len(fc_weights) == num_channels * feature_maps.shape[1] * feature_maps.shape[2]:\n",
    "        weights_reshaped = fc_weights.reshape(num_channels, feature_maps.shape[1], feature_maps.shape[2])\n",
    "        channel_weights = weights_reshaped.mean(axis=(1, 2))\n",
    "    # Option 3: Fallback - use equal weights\n",
    "    else:\n",
    "        channel_weights = np.ones(num_channels)\n",
    "    \n",
    "    # Calculate the weighted sum of feature maps\n",
    "    cam = np.zeros(feature_maps.shape[1:], dtype=np.float32)\n",
    "    for i, w in enumerate(channel_weights):\n",
    "        cam += w * feature_maps[i]\n",
    "    \n",
    "    # Apply ReLU to the CAM\n",
    "    cam = np.maximum(cam, 0)\n",
    "    \n",
    "    # Normalize CAM\n",
    "    cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam) + 1e-8)  # Added epsilon to avoid division by zero\n",
    "    \n",
    "    # Resize CAM to the size of the input image\n",
    "    from skimage.transform import resize\n",
    "    cam = resize(cam, (28, 28))\n",
    "    \n",
    "    return cam, output\n",
    "\n",
    "def show_cam(model, data_loader, device, class_names, num_images=5):\n",
    "    \"\"\"\n",
    "    Show Class Activation Maps for a few images.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: DataLoader with data\n",
    "        device: Device to run model on\n",
    "        class_names: List of class names\n",
    "        num_images: Number of images to show\n",
    "    \"\"\"\n",
    "    # Get a batch of images\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    plt.figure(figsize=(15, num_images * 3))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        image = images[i:i+1]\n",
    "        label = labels[i].item()\n",
    "        \n",
    "        # Generate CAM\n",
    "        cam, output = generate_cam(model, image, label, device)\n",
    "        \n",
    "        # Get predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "        pred = pred.item()\n",
    "        \n",
    "        # Plot original image - handling grayscale properly\n",
    "        plt.subplot(num_images, 3, i*3 + 1)\n",
    "        img = image.cpu().numpy().squeeze()  # This is already a 2D array for grayscale\n",
    "        plt.imshow(img * 0.5 + 0.5, cmap='gray')\n",
    "        plt.title(f\"Original Image\\nTrue: {class_names[label]}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Plot CAM\n",
    "        plt.subplot(num_images, 3, i*3 + 2)\n",
    "        plt.imshow(cam, cmap='jet')\n",
    "        plt.title(\"Class Activation Map\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Plot overlay\n",
    "        plt.subplot(num_images, 3, i*3 + 3)\n",
    "        plt.imshow(img * 0.5 + 0.5, cmap='gray')\n",
    "        plt.imshow(cam, alpha=0.5, cmap='jet')\n",
    "        plt.title(f\"Overlay\\nPred: {class_names[pred]}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Class Activation Maps for {model.__class__.__name__}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.show()\n",
    "\n",
    "# Show CAMs for the best model\n",
    "best_model = baseline_model  # Change this to the best performing model\n",
    "show_cam(best_model, test_loader, device, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Final Summary Statistics and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(comparison_results):\n",
    "    \"\"\"Print final summary statistics of the project.\"\"\"\n",
    "    # Sort models by accuracy\n",
    "    sorted_results = sorted(comparison_results, key=lambda x: x['accuracy'], reverse=True)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"FASHION-MNIST CLASSIFICATION PROJECT SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nModel Performance Comparison (from best to worst):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model':<20} {'Accuracy (%)':<15} {'Parameters':<12} {'Params (MB)':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for result in sorted_results:\n",
    "        name = result['name']\n",
    "        acc = result['accuracy'] * 100\n",
    "        params = result['parameters']\n",
    "        params_mb = params / (1024 * 1024)\n",
    "        \n",
    "        print(f\"{name:<20} {acc:<15.2f} {params:<12,} {params_mb:<12.2f}\")\n",
    "    \n",
    "    print(\"\\nKey Findings:\")\n",
    "    \n",
    "    # Determine best model\n",
    "    best_model = sorted_results[0]['name']\n",
    "    best_acc = sorted_results[0]['accuracy'] * 100\n",
    "    \n",
    "    # Determine most efficient model\n",
    "    efficiency = [(r['name'], r['accuracy'] * 100 / (r['parameters'] / 10**6)) for r in comparison_results]\n",
    "    most_efficient = max(efficiency, key=lambda x: x[1])\n",
    "    \n",
    "    # Print key findings\n",
    "    print(f\"1. Best performing model: {best_model} with {best_acc:.2f}% accuracy\")\n",
    "    print(f\"2. Most parameter-efficient model: {most_efficient[0]} with {most_efficient[1]:.2f}% accuracy per million parameters\")\n",
    "    \n",
    "    # Compare baseline vs. best\n",
    "    baseline = next(r for r in comparison_results if r['name'] == 'Baseline CNN')\n",
    "    baseline_acc = baseline['accuracy'] * 100\n",
    "    improvement = best_acc - baseline_acc\n",
    "    \n",
    "    print(f\"3. Improvement over baseline: +{improvement:.2f}% absolute accuracy\")\n",
    "    \n",
    "    # Effect of techniques\n",
    "    print(\"\\nEffect of Advanced Techniques:\")\n",
    "    \n",
    "    # Dilated convolutions\n",
    "    baseline = next(r for r in comparison_results if r['name'] == 'Baseline CNN')\n",
    "    dilated = next(r for r in comparison_results if r['name'] == 'Dilated CNN')\n",
    "    dilated_effect = (dilated['accuracy'] - baseline['accuracy']) * 100\n",
    "    \n",
    "    print(f\"- Dilated Convolutions: {dilated_effect:+.2f}% absolute accuracy change\")\n",
    "    \n",
    "    # Efficient architecture\n",
    "    efficient = next(r for r in comparison_results if r['name'] == 'Efficient CNN')\n",
    "    efficient_effect = (efficient['accuracy'] - baseline['accuracy']) * 100\n",
    "    efficient_param_reduction = (1 - efficient['parameters'] / baseline['parameters']) * 100\n",
    "    \n",
    "    print(f\"- Efficient Architecture: {efficient_effect:+.2f}% accuracy change, {efficient_param_reduction:.1f}% parameter reduction\")\n",
    "    \n",
    "    # MixUp\n",
    "    if 'MixUp Dilated CNN' in [r['name'] for r in comparison_results]:\n",
    "        mixup = next(r for r in comparison_results if r['name'] == 'MixUp Dilated CNN')\n",
    "        dilated = next(r for r in comparison_results if r['name'] == 'Dilated CNN')\n",
    "        mixup_effect = (mixup['accuracy'] - dilated['accuracy']) * 100\n",
    "        \n",
    "        print(f\"- MixUp Augmentation: {mixup_effect:+.2f}% absolute accuracy change\")\n",
    "    \n",
    "    print(\"\\nConclusions:\")\n",
    "    \n",
    "    # Generate some conclusions based on the results\n",
    "    if best_model == 'MixUp Dilated CNN':\n",
    "        print(\"1. The combination of dilated convolutions and MixUp augmentation yielded the best results,\")\n",
    "        print(\"   demonstrating the value of both architectural improvements and data augmentation.\")\n",
    "    elif best_model == 'Dilated CNN':\n",
    "        print(\"1. Dilated convolutions provided the most significant improvement,\")\n",
    "        print(\"   likely due to the increased receptive field without additional parameters.\")\n",
    "    elif best_model == 'Efficient CNN':\n",
    "        print(\"1. The efficient architecture achieved the best balance of accuracy and model size,\")\n",
    "        print(\"   demonstrating that parameter reduction doesn't necessarily sacrifice performance.\")\n",
    "    \n",
    "    if most_efficient[0] == 'Efficient CNN':\n",
    "        print(\"2. The depthwise separable convolutions in the Efficient CNN proved very parameter-efficient,\")\n",
    "        print(\"   achieving a good accuracy-to-parameter ratio.\")\n",
    "    \n",
    "    print(\"3. The project successfully demonstrated the progression from a baseline model\")\n",
    "    print(\"   to more advanced architectures with meaningful improvements at each step.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Print final summary\n",
    "print_summary(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Save the Best Model With Existence Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(model, model_name, save_dir='./models'):\n",
    "    \"\"\"Save the best model for future use.\"\"\"\n",
    "    try:\n",
    "        # Try to create the directory\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save the model\n",
    "        save_path = os.path.join(save_dir, f\"{model_name}.pth\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        \n",
    "        print(f\"Model saved to {save_path}\")\n",
    "        \n",
    "        # Save model architecture info\n",
    "        info_path = os.path.join(save_dir, f\"{model_name}_info.txt\")\n",
    "        with open(info_path, 'w') as f:\n",
    "            f.write(f\"Model: {model.__class__.__name__}\\n\")\n",
    "            f.write(f\"Parameters: {model.count_parameters():,}\\n\")\n",
    "            f.write(f\"Architecture:\\n{model}\\n\")\n",
    "        \n",
    "        print(f\"Model info saved to {info_path}\")\n",
    "        \n",
    "    except OSError as e:\n",
    "        print(f\"Warning: Could not save model due to file system restrictions: {e}\")\n",
    "        print(\"Options to save your model:\")\n",
    "        print(\"1. If using VS Code, try using an absolute path\")\n",
    "        print(\"2. Check if your current directory is writable\")\n",
    "        print(\"3. Download the model parameters directly\")\n",
    "        \n",
    "        # Provide a way to download the model directly if in Jupyter\n",
    "        try:\n",
    "            from IPython.display import FileLink, display\n",
    "            import tempfile\n",
    "            \n",
    "            # Create a temporary file\n",
    "            temp_dir = tempfile.gettempdir()\n",
    "            temp_path = os.path.join(temp_dir, f\"{model_name}.pth\")\n",
    "            \n",
    "            # Save model to temp location\n",
    "            torch.save(model.state_dict(), temp_path)\n",
    "            print(f\"Model temporarily saved to {temp_path}\")\n",
    "            \n",
    "            # Provide download link\n",
    "            print(\"You can download the model using the link below:\")\n",
    "            display(FileLink(temp_path))\n",
    "            \n",
    "        except (ImportError, OSError) as sub_error:\n",
    "            print(f\"Could not provide download link: {sub_error}\")\n",
    "            print(\"Your model is still available in the current session as a variable.\")\n",
    "\n",
    "# Check which models are defined in the current session\n",
    "available_models = []\n",
    "model_names = ['baseline_model', 'dilated_model', 'efficient_model', 'mixup_model']\n",
    "\n",
    "for name in model_names:\n",
    "    if name in globals():\n",
    "        available_models.append((name, globals()[name]))\n",
    "\n",
    "if available_models:\n",
    "    print(\"Available models:\")\n",
    "    for name, model in available_models:\n",
    "        print(f\"- {name}\")\n",
    "    \n",
    "    # Save the first available model (or let user choose)\n",
    "    best_model = available_models[0][1]\n",
    "    best_model_name = \"fashion_mnist_best_model\"\n",
    "    save_best_model(best_model, best_model_name)\n",
    "else:\n",
    "    print(\"No models are defined in the current session.\")\n",
    "    print(\"Please run the cells that define and train the models first.\")\n",
    "    print(\"These are typically cells 6-15 in the notebook.\")\n",
    "\n",
    "print(\"Cell completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
